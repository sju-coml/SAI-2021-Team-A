{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_박승일.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYxtg-xiuOxp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcj3NkHZzNY_"
      },
      "source": [
        "**Residual Unit 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUo5FOD3x609"
      },
      "source": [
        "class ResidualUnit(tf.keras.Model):\n",
        "  def __init__(self, filter_in, filter_out, kernel_size):\n",
        "    super(ResidualUnit, self).__init__()\n",
        "    # batch normalization -> ReLu -> Conv Layer\n",
        "    # 여기서 ReLu 같은 경우는 변수가 없는 Layer이므로 여기서 굳이 initialize 해주지 않는다. (call쪽에서 사용하면 되므로)\n",
        "\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization() #배치정규화\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=\"same\") #Convolution진행\n",
        "\n",
        "    self.bn2 = tf.keras.layers.BatchNormalization() #배치정규화\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=\"same\") #Convolution진행\n",
        "\n",
        "    # identity정의\n",
        "    # X와 위의 과정을 통해 얻은 Feature map과 차원 고려\n",
        "    # 위에서 filter_in과 filter_out이 같아야 한다는 의미\n",
        "    # 하지만, 다를 수 있으므로 아래와 같은 작업을 거친다.\n",
        "\n",
        "    if filter_in == filter_out: #같은경우\n",
        "      self.identity = lambda x: x\n",
        "    else: #차원이 다른 경우\n",
        "      self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding=\"same\")\n",
        "\n",
        "  # 아래에서 batch normalization은 train할때와 inference할 때 사용하는 것이 달라지므로 옵션을 줄것이다.\n",
        "  def call(self, x, training=False, mask=None):\n",
        "    h = self.bn1(x, training=training)\n",
        "    h = tf.nn.relu(h)\n",
        "    h = self.conv1(h)\n",
        "\n",
        "    h = self.bn2(h, training=training)\n",
        "    h = tf.nn.relu(h)\n",
        "    h = self.conv2(h)\n",
        "    return self.identity(x) + h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNQH0Q7ezTQZ"
      },
      "source": [
        "**Residual Layer 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SlsxrCjzHeR"
      },
      "source": [
        "class ResnetLayer(tf.keras.Model):\n",
        "  # 아래 arg 중 filter_in : 처음 입력되는 filter 개수를 의미\n",
        "  # Resnet Layer는 Residual unit이 여러개가 있게끔해주는것이므로\n",
        "  # filters : [32, 32, 32, 32]는 32에서 32로 Residual unit이 연결되는 형태\n",
        "  def __init__(self, filter_in, filters, kernel_size):\n",
        "    super(ResnetLayer, self).__init__()\n",
        "    self.sequnce = list()\n",
        "    # [16] + [32, 32, 32]\n",
        "    # 아래는 list의 length가 더 작은 것을 기준으로 zip이 되어서 돌아가기 때문에\n",
        "    # 앞의 list의 마지막 element 32는 무시된다.\n",
        "    # zip([16, 32, 32, 32], [32, 32, 32])\n",
        "    for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
        "      self.sequnce.append(ResidualUnit(f_in, f_out, kernel_size))\n",
        "\n",
        "  def call(self, x, training=False, mask=None):\n",
        "    for unit in self.sequnce:\n",
        "      # 위의 batch normalization에서 training이 쓰였기에 여기서 넘겨 주어야 한다.\n",
        "      x = unit(x, training=training)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZMaSdhpzil4"
      },
      "source": [
        "**모델 설정**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_gYkXrzzlpA"
      },
      "source": [
        "class ResNet(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.conv1 = tf.keras.layers.Conv2D(8, (3,3), padding=\"same\", activation=\"relu\") # 28X28X8\n",
        "\n",
        "    self.res1 = ResnetLayer(8, (16, 16), (3, 3)) # 28X28X16\n",
        "    self.pool1 = tf.keras.layers.MaxPool2D((2,2)) # 14X14X16\n",
        "\n",
        "    self.res2 = ResnetLayer(16, (32, 32), (3, 3)) # 14X14X32\n",
        "    self.pool2 = tf.keras.layers.MaxPool2D((2,2)) # 7X7X32\n",
        "\n",
        "    self.res3 = ResnetLayer(32, (64, 64), (3, 3)) # 7X7X64\n",
        "\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\")\n",
        "    self.dense2 = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "\n",
        "  def call(self, x, training=False, mask=None):\n",
        "    x = self.conv1(x)\n",
        "\n",
        "    x = self.res1(x, training=training)\n",
        "    x = self.pool1(x)\n",
        "    x = self.res2(x, training=training)\n",
        "    x = self.pool2(x)\n",
        "    x = self.res3(x, training=training)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.dense1(x)\n",
        "    return self.dense2(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M36-1gQ_zstd"
      },
      "source": [
        "Train, Test loop 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L173n87yzqpC"
      },
      "source": [
        "# Implement training loop\n",
        "@tf.function\n",
        "def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True 꼭 넣어주기!!\n",
        "    predictions = model(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "# Implement algorithm test\n",
        "@tf.function\n",
        "def test_step(model, images, labels, loss_object, test_loss, test_accuracy):\n",
        "  # training=False 꼭 넣어주기!!\n",
        "  predictions = model(images, training=False)\n",
        "\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtAoqjbKz7VQ"
      },
      "source": [
        "**데이터 셋 준비**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WkfWpeaz3DX",
        "outputId": "3baff9f8-d649-4284-ccdb-37af8aef43de"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = x_train[..., tf.newaxis].astype(np.float32)\n",
        "x_test = x_test[..., tf.newaxis].astype(np.float32)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RoDUgLWz_KX"
      },
      "source": [
        "**학습 환경 설정**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0h64D1H0Bi0"
      },
      "source": [
        "# 모델 생성\n",
        "model = ResNet()\n",
        "\n",
        "# 손실함수 정의 및 최적화 기법 정의\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# 평가지표 정의\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9RbK7P50IaL"
      },
      "source": [
        "**학습 실행 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JRaW_OR0Ht_",
        "outputId": "898e6f60-7edc-4347-e534-d846fc561c6b"
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "  for images, labels in train_ds:\n",
        "    train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
        "\n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy)\n",
        "\n",
        "  template = \"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}\"\n",
        "  print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result() * 100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result() * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.13984577357769012, Accuracy: 95.96666717529297, Test Loss: 0.08611265569925308, Test Accuracy: 97.52999877929688\n",
            "Epoch 2, Loss: 0.10237626731395721, Accuracy: 97.07083129882812, Test Loss: 0.06297999620437622, Test Accuracy: 98.08999633789062\n",
            "Epoch 3, Loss: 0.08494842797517776, Accuracy: 97.5616683959961, Test Loss: 0.06293836236000061, Test Accuracy: 98.22000122070312\n",
            "Epoch 4, Loss: 0.07394673675298691, Accuracy: 97.88333129882812, Test Loss: 0.05893087387084961, Test Accuracy: 98.2750015258789\n",
            "Epoch 5, Loss: 0.06668729335069656, Accuracy: 98.08899688720703, Test Loss: 0.057038161903619766, Test Accuracy: 98.37800598144531\n",
            "Epoch 6, Loss: 0.060232434421777725, Accuracy: 98.27361297607422, Test Loss: 0.053865354508161545, Test Accuracy: 98.47833251953125\n",
            "Epoch 7, Loss: 0.055842917412519455, Accuracy: 98.40166473388672, Test Loss: 0.0502890944480896, Test Accuracy: 98.5728530883789\n",
            "Epoch 8, Loss: 0.05169401690363884, Accuracy: 98.51478576660156, Test Loss: 0.04809664934873581, Test Accuracy: 98.64749908447266\n",
            "Epoch 9, Loss: 0.04848707839846611, Accuracy: 98.61018371582031, Test Loss: 0.05283737927675247, Test Accuracy: 98.54000091552734\n",
            "Epoch 10, Loss: 0.04568936675786972, Accuracy: 98.69300079345703, Test Loss: 0.05077522620558739, Test Accuracy: 98.593994140625\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}