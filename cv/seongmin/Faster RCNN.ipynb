{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# utils에는 IoU 계산, NMS, bounding box -> loc 변환, loc -> bounding box 변환을 돕는 기능들이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      ")\n",
      "torch.Size([1, 512, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "image = torch.zeros((1, 3, 800, 800)).float()\n",
    "image_size = (800, 800)\n",
    "# 샘플 이미지 800x800\n",
    "\n",
    "# bbox -> y1, x1, y2, x2\n",
    "bbox = torch.FloatTensor([[20, 30, 400, 500], [300, 400, 500, 600]])\n",
    "labels = torch.LongTensor([6, 8])\n",
    "\n",
    "# bounding box는 2개\n",
    "\n",
    "sub_sample = 16\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "req_features = vgg16.features[:30]\n",
    "print(req_features)\n",
    "output_map = req_features(image)\n",
    "print(output_map.shape)\n",
    "\n",
    "# 원본 이미지의 feature 추출에 사용할 cnn 모델로 vgg-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -45.254834    -90.50966799   45.254834     90.50966799]\n",
      " [ -64.          -64.           64.           64.        ]\n",
      " [ -90.50966799  -45.254834     90.50966799   45.254834  ]\n",
      " [ -90.50966799 -181.01933598   90.50966799  181.01933598]\n",
      " [-128.         -128.          128.          128.        ]\n",
      " [-181.01933598  -90.50966799  181.01933598   90.50966799]\n",
      " [-181.01933598 -362.03867197  181.01933598  362.03867197]\n",
      " [-256.         -256.          256.          256.        ]\n",
      " [-362.03867197 -181.01933598  362.03867197  181.01933598]]\n"
     ]
    }
   ],
   "source": [
    "# anchor 생성\n",
    "\n",
    "anchor_scale = [8, 16, 32]\n",
    "ratio = [0.5, 1, 2] # H/W\n",
    "\n",
    "len_anchor_scale = len(anchor_scale)\n",
    "len_ratio = len(ratio)\n",
    "len_anchor_template = len_anchor_scale * len_ratio\n",
    "anchor_template = np.zeros((9, 4))\n",
    "\n",
    "for idx, scale in enumerate(anchor_scale):\n",
    "    h = scale * np.sqrt(ratio) * sub_sample\n",
    "    w = scale / np.sqrt(ratio) * sub_sample\n",
    "    y1 = -h/2\n",
    "    x1 = -w/2\n",
    "    y2 = h/2\n",
    "    x2 = w/2\n",
    "    anchor_template[idx*len_ratio:(idx+1)*len_ratio, 0] = y1\n",
    "    anchor_template[idx*len_ratio:(idx+1)*len_ratio, 1] = x1\n",
    "    anchor_template[idx*len_ratio:(idx+1)*len_ratio, 2] = y2\n",
    "    anchor_template[idx*len_ratio:(idx+1)*len_ratio, 3] = x2\n",
    "\n",
    "print(anchor_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 2)\n"
     ]
    }
   ],
   "source": [
    "feature_map_size = (50, 50)\n",
    "# The first center coors is (8, 8)\n",
    "ctr_y = np.arange(8, 800, 16)\n",
    "ctr_x = np.arange(8, 800, 16)\n",
    "\n",
    "ctr = np.zeros((*feature_map_size, 2))\n",
    "for idx, y in enumerate(ctr_y):\n",
    "    ctr[idx, :, 0] = y\n",
    "    ctr[idx, :, 1] = ctr_x\n",
    "print(ctr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "anchors = np.zeros((*feature_map_size, 9, 4))\n",
    "\n",
    "for idx_y in range(feature_map_size[0]):\n",
    "    for idx_x in range(feature_map_size[1]):\n",
    "        anchors[idx_y, idx_x] = (ctr[idx_y, idx_x] + anchor_template.reshape(-1, 2, 2)).reshape(-1, 4)\n",
    "\n",
    "anchors = anchors.reshape(-1, 4)\n",
    "print(anchors.shape) # (22500, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "# 이미지를 넘어가는 박스는 사용을 못하므로 제외\n",
    "\n",
    "valid_index = np.where((anchors[:, 0] >= 0)\n",
    "                      &(anchors[:, 1] >= 0)\n",
    "                      &(anchors[:, 2] <= 800)\n",
    "                      &(anchors[:, 3] <= 800))[0]\n",
    "print(valid_index.shape) # 8940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940, 4)\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "valid_labels = np.empty((valid_index.shape[0],), dtype=np.int32)\n",
    "valid_labels.fill(-1)\n",
    "\n",
    "valid_anchors = anchors[valid_index]\n",
    "\n",
    "print(valid_anchors.shape) # (8940,4)\n",
    "print(bbox.shape) # torch.Size([2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'bbox_iou'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d3a275deaaf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mious\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbbox_iou\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_anchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# anchor 8940 : bbox 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpos_iou_thres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mneg_iou_thred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'utils' has no attribute 'bbox_iou'"
     ]
    }
   ],
   "source": [
    "ious = bbox_iou(valid_anchors, bbox.numpy()) # anchor 8940 : bbox 2\n",
    "# bbox_iou가 안불러지는 에러.. 아니 난 그대로 따라 했는데 어째서\n",
    "\n",
    "pos_iou_thres = 0.7\n",
    "neg_iou_thred = 0.3\n",
    "\n",
    "# Scenario A 논문 그대로의 방법\n",
    "anchor_max_iou = np.amax(ious, axis=1)\n",
    "pos_iou_anchor_label = np.where(anchor_max_iou >= pos_iou_thres)[0]\n",
    "neg_iou_anchor_label = np.where(anchor_max_iou < neg_iou_thred)[0]\n",
    "valid_labels[pos_iou_anchor_label] = 1\n",
    "valid_labels[neg_iou_anchor_label] = 0\n",
    "\n",
    "# Scenario B 대부분 0.7 이상 iou가 별로 없어서 박스별 iou가 최대값인 애들로 positive la\n",
    "gt_max_iou = np.amax(ious, axis=0)\n",
    "gt_max_iou_anchor_label = np.where(ious == gt_max_iou)[0]\n",
    "print(gt_max_iou_anchor_label)\n",
    "valid_labels[gt_max_iou_anchor_label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive와 negative를 합쳐 256개만 남기고 제외\n",
    "# positive가 128개가 안되면 나머지는 negative로 채움\n",
    "\n",
    "n_sample_anchors = 256\n",
    "pos_ratio = 0.5\n",
    "\n",
    "total_n_pos = len(np.where(valid_labels == 1)[0])\n",
    "n_pos_sample = n_sample_anchors*pos_ratio if total_n_pos > n_sample_anchors*pos_ratio else total_n_pos\n",
    "n_neg_sample = n_sample_anchors - n_pos_sample\n",
    "\n",
    "pos_index = np.where(valid_labels == 1)[0]\n",
    "if len(pos_index) > n_sample_anchors*pos_ratio:\n",
    "    disable_index = np.random.choice(pos_index, size=len(pos_index)-n_pos_sample, replace=False)\n",
    "    valid_labels[disable_index] = -1\n",
    "\n",
    "neg_index = np.where(valid_labels == 0)[0]\n",
    "disable_index = np.random.choice(neg_index, size=len(neg_index) - n_neg_sample, replace=False)\n",
    "valid_labels[disable_index] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ious에서 앵커별로 어떤 박스가 iou가 높은지 확인\n",
    "# (0.37312, 0.38272) 이면 1, (0.38272, 0.37312) 이면 0\n",
    "# 이렇게하면, 1 0 1 0 0 0 0 1 0, ... 이라는 8940개의 배열이 생기게 됩니다.\n",
    "# 이 index로 box값들을 하나하나 할당해서 8940, 4의 배열을 만듭니다.\n",
    "\n",
    "argmax_iou = np.argmax(ious, axis=1)\n",
    "max_iou_box = bbox[argmax_iou].numpy()\n",
    "print(max_iou_box.shape) # 8940, 4\n",
    "print(valid_anchors.shape) # 8940, 4\n",
    "\n",
    "anchor_loc_format_target = format_loc(valid_anchors, max_iou_box)\n",
    "print(anchor_loc_format_target.shape) # 8940, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 라벨링\n",
    "\n",
    "anchor_target_labels = np.empty((len(anchors),), dtype=np.int32)\n",
    "anchor_target_format_locations = np.zeros((len(anchors), 4), dtype=np.float32)\n",
    "\n",
    "anchor_target_labels.fill(-1)\n",
    "anchor_target_labels[valid_index] = valid_labels\n",
    "\n",
    "anchor_target_format_locations[valid_index] = anchor_loc_format_target\n",
    "\n",
    "print(anchor_target_labels.shape) # 22500,\n",
    "print(anchor_target_format_locations.shape) # 22500, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RPN\n",
    "\n",
    "mid_channel = 512\n",
    "in_channel = 512\n",
    "n_anchor = 9\n",
    "\n",
    "# VGG의 출력 채널이 512이므로 입력 채널을 512로 설정\n",
    "\n",
    "conv1 = nn.Conv2d(in_channel, mid_channel, 3, 1, 1)\n",
    "reg_layer = nn.Conv2d(mid_channel, n_anchor*4, 1, 1, 0)\n",
    "cls_layer = nn.Conv2d(mid_channel, n_anchor*2, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = conv1(output_map)\n",
    "anchor_pred_format_locations = reg_layer(x)\n",
    "anchor_pred_scores = cls_layer(x)\n",
    "\n",
    "print(anchor_pred_format_locations.shape) # torch.Size([1, 36, 50, 50])\n",
    "print(anchor_pred_scores.shape) # torch.Size([1, 18, 50, 50])\n",
    "\n",
    "# VGG에서 얻은 feature를 통과시켜서 location과 class 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_pred_format_locations = anchor_pred_format_locations.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "anchor_pred_scores = anchor_pred_scores.permute(0, 2, 3, 1).contiguous().view(1, -1, 2)\n",
    "objectness_pred_scores = anchor_pred_scores[:, :, 1]\n",
    "\n",
    "# ground truth로 만든 앵커와 비교하기 위해 형태 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anchor_target_labels.shape)\n",
    "print(anchor_target_format_locations.shape)\n",
    "print(anchor_pred_scores.shape)\n",
    "print(anchor_pred_format_locations.shape)\n",
    "\n",
    "gt_rpn_format_locs = torch.from_numpy(anchor_target_format_locations)\n",
    "gt_rpn_scores = torch.from_numpy(anchor_target_labels)\n",
    "\n",
    "rpn_format_locs = anchor_pred_format_locations[0]\n",
    "rpn_scores = anchor_pred_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Object or not loss/  object인지 아닌지 cross entropy loss\n",
    "rpn_cls_loss = F.cross_entropy(rpn_scores, gt_rpn_scores.long(), ignore_index=-1)\n",
    "print(rpn_cls_loss)\n",
    "\n",
    "\n",
    "####### location loss 실제 object인 것만 loss 계산\n",
    "mask = gt_rpn_scores > 0\n",
    "mask_target_format_locs = gt_rpn_format_locs[mask]\n",
    "mask_pred_format_locs = rpn_format_locs[mask]\n",
    "\n",
    "print(mask_target_format_locs.shape)\n",
    "print(mask_pred_format_locs.shape)\n",
    "\n",
    "x = torch.abs(mask_target_format_locs - mask_pred_format_locs)\n",
    "rpn_loc_loss = ((x<0.5).float()*(x**2)*0.5 + (x>0.5).float()*(x-0.5)).sum()\n",
    "print(rpn_loc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_lambda = 10\n",
    "N_reg = mask.float().sum()\n",
    "\n",
    "rpn_loss = rpn_cls_loss + rpn_lambda / N_reg * rpn_loc_loss\n",
    "print(rpn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast rcnn\n",
    "\n",
    "nms_thresh = 0.7\n",
    "n_train_pre_nms = 12000\n",
    "n_train_post_nms = 2000\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300\n",
    "min_size = 16\n",
    "\n",
    "print(anchors.shape) # 22500, 4\n",
    "print(anchor_pred_format_locations.shape) # 22500, 4\n",
    "\n",
    "rois = deformat_loc(anchors=anchors, formatted_base_anchor=anchor_pred_format_locations[0].data.numpy())\n",
    "print(rois.shape) # 22500, 4\n",
    "\n",
    "print(rois)\n",
    "\n",
    "# nms로 같은 클래스 정보를 가지는 박스끼리 iou를 비교해 중복되는 것은 제외\n",
    "# 최정족으로 2000개의 proposal만 남게 된다. 이로 fast rcnn 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois[:, 0:4:2] = np.clip(rois[:, 0:4:2], a_min=0, a_max=image_size[0])\n",
    "rois[:, 1:4:2] = np.clip(rois[:, 1:4:2], a_min=0, a_max=image_size[1])\n",
    "print(rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = rois[:, 2] - rois[:, 0]\n",
    "w = rois[:, 3] - rois[:, 1]\n",
    "\n",
    "valid_index = np.where((h>min_size)&(w>min_size))[0]\n",
    "valid_rois = rois[valid_index]\n",
    "valid_scores = objectness_pred_scores[0][valid_index].data.numpy()\n",
    "# box 크기가 16보다 작은 것은 제외하고 object score를 기주으로 정렬\n",
    "\n",
    "valid_score_order = valid_scores.ravel().argsort()[::-1]\n",
    "\n",
    "pre_train_valid_score_order = valid_score_order[:n_train_pre_nms]\n",
    "pre_train_valid_rois = valid_rois[pre_train_valid_score_order]\n",
    "pre_train_valid_scores = valid_scores[pre_train_valid_score_order]\n",
    "\n",
    "print(pre_train_valid_rois.shape) # 12000, 4\n",
    "print(pre_train_valid_scores.shape) # 12000,\n",
    "print(pre_train_valid_score_order.shape) # 12000,\n",
    "# nms 적용 전 12000개만 가져오고\n",
    "\n",
    "keep_index = nms(rois=pre_train_valid_rois, scores=pre_train_valid_scores, nms_thresh=nms_thresh)\n",
    "post_train_valid_rois = pre_train_valid_rois[keep_index][:n_train_post_nms]\n",
    "post_train_valid_scores = pre_train_valid_scores[keep_index][:n_train_post_nms]\n",
    "print(post_train_valid_rois.shape) # 2000, 4\n",
    "print(post_train_valid_scores.shape) # 2000, \n",
    "# nms 적용하여 2000개만 남김"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = torch.from_numpy(post_sample_rois).float()\n",
    "print(rois.shape) # 128, 4\n",
    "# roi_indices = torch.zeros((len(rois),1), dtype=torch.float32)\n",
    "# print(rois.shape, roi_indices.shape)\n",
    "\n",
    "# indices_and_rois = torch.cat([roi_indices, rois], dim=1)\n",
    "# print(indices_and_rois.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = torch.from_numpy(post_sample_rois).float()\n",
    "print(rois.shape) # 128, 4\n",
    "# roi를 토치로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size)\n",
    "\n",
    "# correspond to feature map\n",
    "rois.mul_(1/16.0)\n",
    "rois = rois.long()\n",
    "\n",
    "output = []\n",
    "num_rois = len(rois)\n",
    "for roi in rois:\n",
    "    roi_feature = output_map[..., roi[0]:roi[2]+1, roi[1]:roi[3]+1]\n",
    "    output.append(adaptive_max_pool(roi_feature))\n",
    "output = torch.cat(output, 0)\n",
    "print(output.shape) # 128, 512, 7, 7\n",
    "\n",
    "output_ROI_pooling = output.view(output.size(0), -1)\n",
    "print(output_ROI_pooling.shape) # 128, 25088\n",
    "\n",
    "# roi pooling을 통해 고정된 크기로 추출 후 일자로 펴면 128, 25088 의 배열이 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_head = nn.Sequential(nn.Linear(25088, 4096),\n",
    "                        nn.Linear(4096, 4096))\n",
    "\n",
    "cls_loc = nn.Linear(4096, 21*4)\n",
    "cls_loc.weight.data.normal_(0, 0.01)\n",
    "cls_loc.bias.data.zero_()\n",
    "\n",
    "cls_score = nn.Linear(4096, 21)\n",
    "cls_score.weight.data.normal_(0, 0.01)\n",
    "cls_score.bias.data.zero_()\n",
    "\n",
    "x = roi_head(output_ROI_pooling)\n",
    "roi_cls_loc = cls_loc(x)\n",
    "roi_cls_score = cls_score(x)\n",
    "\n",
    "print(roi_cls_loc.shape, roi_cls_score.shape) # 128, 84 / 128, 21\n",
    "\n",
    "# 최종적으로 fully connected layer를 거쳐 20 + 1 로 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
